## OuteTTS

ğŸŒ [Website (outeai.com)](https://www.outeai.com) | ğŸ¤— [Hugging Face](https://huggingface.co/OuteAI) | ğŸ’¬ [Discord](https://discord.gg/vyBM87kAmf) | ğ• [X (Twitter)](https://twitter.com/OuteAI) | ğŸ“° [Blog](https://www.outeai.com/blog)

[![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Llama_OuteTTS_1.0_1B-blue)](https://huggingface.co/OuteAI/Llama-OuteTTS-1.0-1B)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Llama_OuteTTS_1.0_0.6B-blue)](https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B)
[![PyPI](https://img.shields.io/badge/PyPI-outetts-5c6c7a)](https://pypi.org/project/outetts/)
[![npm](https://img.shields.io/badge/npm-outetts-734440)](https://www.npmjs.com/package/outetts)

## Compatibility

#### OuteTTS supports the following backends:  

| **Backend** | **Language** | **Installation** | **Model Version Support** |
|-----------------------------|---------|----------------------------|---------|  
| [Llama.cpp Python Bindings](https://github.com/abetlen/llama-cpp-python) | Python | âœ… Installed by default | `All` |
| [Llama.cpp Server](https://github.com/ggml-org/llama.cpp/tree/master/tools/server) | Python | âœ… Installed by default | `All` |
| [Llama.cpp Server Async (Batched)](https://github.com/ggml-org/llama.cpp/tree/master/tools/server) | Python | âœ… Installed by default | `1.0` |
| [Hugging Face Transformers](https://github.com/huggingface/transformers) | Python | âœ… Installed by default | `All` | 
| [ExLlamaV2](https://github.com/turboderp/exllamav2) | Python | âŒ Requires manual installation | `All` |
| [ExLlamaV2 Async (Batched)](https://github.com/turboderp/exllamav2) | Python | âŒ Requires manual installation | `1.0` |
| [VLLM (Batched) **Experimental support**](https://github.com/vllm-project/vllm) | Python | âŒ Requires manual installation | `1.0` |
| [Transformers.js](https://github.com/huggingface/transformers.js) | JavaScript | NPM package | `0.2` |

### âš¡ **Batched RTF Benchmarks**  
Tested with **NVIDIA L40S GPU** 

![rtf](docs/assets/rtf.png)

## Installation

### OuteTTS Installation Guide

OuteTTS now installs the llama.cpp Python bindings by default. Therefore, you must specify the installation based on your hardware. For more detailed instructions on building llama.cpp, refer to the following resources: [llama.cpp Build](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md) and [llama.cpp Python](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends)

### Pip:

<details open>
<summary>Transformers + llama.cpp CPU</summary>

```bash
pip install outetts --upgrade
```
</details>

<details>
<summary>Transformers + llama.cpp CUDA (NVIDIA GPUs)</summary>
For systems with NVIDIA GPUs and CUDA installed:

```bash
CMAKE_ARGS="-DGGML_CUDA=on" pip install outetts --upgrade
```

</details>

<details>
<summary>Transformers + llama.cpp ROCm/HIP (AMD GPUs)</summary>
For systems with AMD GPUs and ROCm (specify your DAMDGPU_TARGETS) installed:

```bash
CMAKE_ARGS="-DGGML_HIPBLAS=on" pip install outetts --upgrade
```

</details>

<details>
<summary>Transformers + llama.cpp Vulkan (Cross-platform GPU)</summary>
For systems with Vulkan support:

```bash
CMAKE_ARGS="-DGGML_VULKAN=on" pip install outetts --upgrade
```
</details>

<details>
<summary>Transformers + llama.cpp Metal (Apple Silicon/Mac)</summary>
For macOS systems with Apple Silicon or compatible GPUs:

```bash
CMAKE_ARGS="-DGGML_METAL=on" pip install outetts --upgrade
```
</details>

## Usage

## ğŸ“š Documentation

For a complete usage guide, refer to the interface documentation here: 

[![Documentation](https://img.shields.io/badge/ğŸ“–_Read_The_Docs-Interface_Guide-blue?style=for-the-badge)](https://github.com/edwko/OuteTTS/blob/main/docs/interface_usage.md)

### Basic Usage

> [!TIP]
> Currently, only **one default English voice** is available for testing.
>
> You can easily create your own speaker profiles in just a few lines by following this guide:
>
> ğŸ‘‰ [Creating Custom Speaker Profiles](https://github.com/edwko/OuteTTS/blob/main/docs/interface_usage.md#creating-custom-speaker-profiles)

```python
import outetts

# Initialize the interface
interface = outetts.Interface(
    config=outetts.ModelConfig.auto_config(
        model=outetts.Models.VERSION_1_0_SIZE_1B,
        # For llama.cpp backend
        backend=outetts.Backend.LLAMACPP,
        quantization=outetts.LlamaCppQuantization.FP16
        # For transformers backend
        # backend=outetts.Backend.HF,
    )
)

# Load the default speaker profile
speaker = interface.load_default_speaker("EN-FEMALE-1-NEUTRAL")

# Or create your own speaker profiles in seconds and reuse them instantly
# speaker = interface.create_speaker("path/to/audio.wav")
# interface.save_speaker(speaker, "speaker.json")
# speaker = interface.load_speaker("speaker.json")

# Generate speech
output = interface.generate(
    config=outetts.GenerationConfig(
        text="Hello, how are you doing?",
        speaker=speaker,
    )
)

# Save to file
output.save("output.wav")
```

## External Libraries with OuteTTS Model Support

These are third-party tools that support running the OuteTTS model.

| **Library** | **Language** | **Model Version Support** |
|-------------|--------------|----------------------------|
| [Llama.cpp TTS Example](https://github.com/ggml-org/llama.cpp/tree/master/tools/tts) | C++ | `0.2` |
| [KoboldCPP](https://github.com/LostRuins/koboldcpp) | C++ | `0.2`, `0.3` |
| [MLX-Audio](https://github.com/Blaizzy/mlx-audio) | Python (MLX) | `1.0` |
| [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp) | C++ | `1.0` |


## Usage Recommendations for OuteTTS version 1.0
> [!IMPORTANT]
> **Important Sampling Considerations**  
> 
> When using OuteTTS version 1.0, it is crucial to use the settings specified in the [Sampling Configuration](#sampling-configuration) section.
> The **repetition penalty implementation** is particularly important - this model requires penalization applied to a **64-token recent window**,
> rather than across the entire context window. Penalizing the entire context will cause the model to produce **broken or low-quality output**.
> 
> To address this limitation, all necessary samplers and patches for all backends are set up automatically in the **outetts** library.
> If using a custom implementation, ensure you correctly implement these requirements.

### Speaker Reference
The model is designed to be used with a speaker reference. Without one, it generates random vocal characteristics, often leading to lower-quality outputs. 
The model inherits the referenced speaker's emotion, style, and accent. 
Therefore, when transcribing to other languages with the same speaker, you may observe the model retaining the original accent. 
For example, if you use a Japanese speaker and continue speech in English, the model may tend to use a Japanese accent.

### Multilingual Application
It is recommended to create a speaker profile in the language you intend to use. This helps achieve the best results in that specific language, including tone, accent, and linguistic features.

While the model supports cross-lingual speech, it still relies on the reference speaker. If the speaker has a distinct accentâ€”such as British Englishâ€”other languages may carry that accent as well.

### Optimal Audio Length
- **Best Performance:** Generate audio around **42 seconds** in a single run (approximately 8,192 tokens). It is recomended not to near the limits of this windows when generating. Usually, the best results are up to 7,000 tokens.
- **Context Reduction with Speaker Reference:** If the speaker reference is 10 seconds long, the effective context is reduced to approximately 32 seconds.

### Temperature Setting Recommendations
Testing shows that a temperature of **0.4** is an ideal starting point for accuracy (with the sampling settings below). However, some voice references may benefit from higher temperatures for enhanced expressiveness or slightly lower temperatures for more precise voice replication.

### Verifying Speaker Encoding
If the cloned voice quality is subpar, check the encoded speaker sample. 

```python
interface.decode_and_save_speaker(speaker=your_speaker, path="speaker.wav")
```

The DAC audio reconstruction model is lossy, and samples with clipping, excessive loudness, or unusual vocal features may introduce encoding issues that impact output quality.

### Sampling Configuration
For optimal results with this TTS model, use the following sampling settings.

| Parameter         | Value    |
|-------------------|----------|
| Temperature       | 0.4      |
| Repetition Penalty| 1.1      |
| **Repetition Range**  | **64**       |
| Top-k             | 40       |
| Top-p             | 0.9      |
| Min-p             | 0.05     |

## Acknowledgments

[DAC (Descript Audio Codec)](https://github.com/descriptinc/descript-audio-codec)

[WavTokenizer](https://github.com/jishengpeng/WavTokenizer)

[CTC Forced Alignment](https://docs.pytorch.org/audio/stable/tutorials/ctc_forced_alignment_api_tutorial.html)

[Uroman](https://github.com/isi-nlp/uroman) *"This project uses the universal romanizer software 'uroman' written by Ulf Hermjakob, USC Information Sciences Institute (2015-2020)"*

[mecab-python3](https://github.com/SamuraiT/mecab-python3)
---

# ğŸš€ í•œêµ­ì–´ ì„¤ì¹˜ ê°€ì´ë“œ (Korean Installation Guide)

## ğŸ“‹ ëª©ì°¨
- [OuteTTS 1.0-0.6B ëª¨ë¸ ì„¤ì¹˜](#outetts-10-06b-ëª¨ë¸-ì„¤ì¹˜)
- [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
- [Tools ë””ë ‰í† ë¦¬ ì„¤ì •](#tools-ë””ë ‰í† ë¦¬-ì„¤ì •)
- [Utils ë””ë ‰í† ë¦¬ ì„¤ì •](#utils-ë””ë ‰í† ë¦¬-ì„¤ì •)
- [ì‚¬ìš© ì˜ˆì œ](#ì‚¬ìš©-ì˜ˆì œ)

## OuteTTS 1.0-0.6B ëª¨ë¸ ì„¤ì¹˜

### 1. ê¸°ë³¸ ì„¤ì¹˜ (CPU)
```bash
pip install outetts --upgrade
```

### 2. GPUë³„ ì„¤ì¹˜ ì˜µì…˜

#### NVIDIA GPU (CUDA)
```bash
CMAKE_ARGS="-DGGML_CUDA=on" pip install outetts --upgrade
```

#### AMD GPU (ROCm/HIP)
```bash
CMAKE_ARGS="-DGGML_HIPBLAS=on" pip install outetts --upgrade
```

#### Apple Silicon/Mac (Metal)
```bash
CMAKE_ARGS="-DGGML_METAL=on" pip install outetts --upgrade
```

#### Vulkan (í¬ë¡œìŠ¤í”Œë«í¼)
```bash
CMAKE_ARGS="-DGGML_VULKAN=on" pip install outetts --upgrade
```

## í™˜ê²½ ì„¤ì •

### 1. í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±

`.env.example` íŒŒì¼ì„ `.env`ë¡œ ë³µì‚¬í•˜ì—¬ ì„¤ì •:

```bash
cp .env.example .env
```

### 2. `.env` íŒŒì¼ ìˆ˜ì •

ì£¼ìš” ì„¤ì • í•­ëª©:

```bash
# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL=postgresql://username:password@localhost:5432/dbname

# OuteTTS ëª¨ë¸ ì„¤ì •
OUTETTS_MODEL_VERSION=1.0
OUTETTS_MODEL_SIZE=0.6B  # ë˜ëŠ” 1B
OUTETTS_BACKEND=LLAMACPP
OUTETTS_QUANTIZATION=FP16

# AI ì„œë¹„ìŠ¤ ì„¤ì •
AI_DEVICE=cuda              # GPU ì‚¬ìš© ì‹œ 'cuda', CPU ì‚¬ìš© ì‹œ 'cpu'
AI_DTYPE=bf16               # bf16, fp16, ë˜ëŠ” fp32
AI_LOW_VRAM=true            # GPU ë©”ëª¨ë¦¬ê°€ 2GB ì´í•˜ì¼ ê²½ìš° true
AI_TEMP=0.4                 # ì˜¨ë„ ì„¤ì • (0.4 ê¶Œì¥)
AI_SEEDS_POOL=12            # ì‹œë“œ í’€ í¬ê¸°
AI_VAL_SAMPLE=3             # ê²€ì¦ ìƒ˜í”Œ í¬ê¸°

# AI ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí¬
AI_SERVICE_HOST=0.0.0.0
AI_SERVICE_PORT=8777

# ì‘ì—…ì ì„¤ì •
CPU_WORKERS=2
GPU_WORKERS=1
SYNTH_WORKERS=1
EVAL_WORKERS=1
```

## Tools ë””ë ‰í† ë¦¬ ì„¤ì •

`tools` ë””ë ‰í† ë¦¬ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìœ í‹¸ë¦¬í‹°ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

### 1. AI Service (`ai_service.py`)
- **ê¸°ëŠ¥**: OuteTTSë¥¼ ìœ„í•œ ì›¹ API ì„œë²„
- **ì‚¬ìš©ë²•**:
  ```bash
  python tools/ai_service.py
  ```
- **API ì—”ë“œí¬ì¸íŠ¸**:
  - `GET /health`: ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
  - `POST /run`: TTS ì‘ì—… ì‹¤í–‰

### 2. Database Utilities
- **`db_check.py`**: ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
- **`db_check_best.py`**: ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì²´í¬
- **`db_util.py`**: ë°ì´í„°ë² ì´ìŠ¤ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

### 3. Job Selector (`job_selector.py`)
- **ê¸°ëŠ¥**: ì‘ì—… ì„ íƒ ë° ê´€ë¦¬ ë„êµ¬

## Utils ë””ë ‰í† ë¦¬ ì„¤ì •

`utils` ë””ë ‰í† ë¦¬ì—ëŠ” í•µì‹¬ ì²˜ë¦¬ ëª¨ë“ˆë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

### 1. Inference (`Inference.py`)
- **ê¸°ëŠ¥**: LoRA ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•œ ì¶”ë¡ 
- **ì‚¬ìš© ì˜ˆì œ**:
  ```python
  from utils.Inference import LoraInference

  # ì˜¤ë””ì˜¤ ë””ë ‰í† ë¦¬ë¡œë¶€í„° ì¶”ë¡ 
  inference = LoraInference.from_audio_dir(
      '/path/to/audio',
      text='ìƒì„±í•  í…ìŠ¤íŠ¸',
      n_candidates=5
  )
  inference.synthesize()
  ```

### 2. Processing (`Processing.py`)
- **ê¸°ëŠ¥**: ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¤€ë¹„

### 3. Book TTS (`book_tts.py`)
- **ê¸°ëŠ¥**: ì±… ì „ì²´ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
- **ì‚¬ìš© ì˜ˆì œ**:
  ```python
  from utils.book_tts import synthesize_chapter

  synthesize_chapter(
      audio_dir='datas/wavs/speaker',
      text_lines=['ì²« ë²ˆì§¸ ë¬¸ì¥', 'ë‘ ë²ˆì§¸ ë¬¸ì¥'],
      n_candidates_per_sentence=1
  )
  ```

### 4. LoRA Training (`lora.py`)
- **ê¸°ëŠ¥**: LoRA íŒŒì¸íŠœë‹ í•™ìŠµ
- **ì„¤ì •**: `lora_hparams.json`ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •

### 5. Memory Monitor (`memory_monitor.py`)
- **ê¸°ëŠ¥**: GPU/CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

### 6. Convert (`convert.py`)
- **ê¸°ëŠ¥**: ì˜¤ë””ì˜¤ íŒŒì¼ í˜•ì‹ ë³€í™˜ ìœ í‹¸ë¦¬í‹°

## ì‚¬ìš© ì˜ˆì œ

### 1. ê¸°ë³¸ TTS ìƒì„±

```python
import outetts

# ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”
interface = outetts.Interface(
    config=outetts.ModelConfig.auto_config(
        model=outetts.Models.VERSION_1_0_SIZE_06B,  # 0.6B ëª¨ë¸
        backend=outetts.Backend.LLAMACPP,
        quantization=outetts.LlamaCppQuantization.FP16
    )
)

# ê¸°ë³¸ ìŠ¤í”¼ì»¤ í”„ë¡œí•„ ë¡œë“œ
speaker = interface.load_default_speaker("EN-FEMALE-1-NEUTRAL")

# ìŒì„± ìƒì„±
output = interface.generate(
    config=outetts.GenerationConfig(
        text="ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤!",
        speaker=speaker,
    )
)

# íŒŒì¼ë¡œ ì €ì¥
output.save("output.wav")
```

### 2. ì»¤ìŠ¤í…€ ìŠ¤í”¼ì»¤ ìƒì„±

```python
# ì˜¤ë””ì˜¤ íŒŒì¼ë¡œë¶€í„° ìŠ¤í”¼ì»¤ í”„ë¡œí•„ ìƒì„±
speaker = interface.create_speaker("path/to/audio.wav")

# ìŠ¤í”¼ì»¤ í”„ë¡œí•„ ì €ì¥
interface.save_speaker(speaker, "my_speaker.json")

# ì €ì¥ëœ ìŠ¤í”¼ì»¤ í”„ë¡œí•„ ë¡œë“œ
speaker = interface.load_speaker("my_speaker.json")
```

### 3. AI Service API ì‚¬ìš©

```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
python tools/ai_service.py

# API í˜¸ì¶œ ì˜ˆì œ (curl)
curl -X POST http://localhost:8777/run \
  -H "Content-Type: application/json" \
  -d '{
    "action": "infer",
    "audio_dir": "/path/to/audio",
    "text": "ìƒì„±í•  í…ìŠ¤íŠ¸",
    "n_candidates": 3,
    "evaluate": true
  }'
```

## ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ì •

### ì˜¨ë„ (Temperature)
- **ê¶Œì¥ê°’**: 0.4
- **ë‚®ì€ ê°’** (0.1-0.3): ë” ì •í™•í•œ ìŒì„± ë³µì œ
- **ë†’ì€ ê°’** (0.5-0.7): ë” í‘œí˜„ë ¥ ìˆëŠ” ìŒì„±

### Low VRAM ëª¨ë“œ
- GPU ë©”ëª¨ë¦¬ê°€ **2GB ì´í•˜**ì¸ ê²½ìš°: `AI_LOW_VRAM=true`
- GPU ë©”ëª¨ë¦¬ê°€ **2GB ì´ìƒ**ì¸ ê²½ìš°: `AI_LOW_VRAM=false`

### Sampling ì„¤ì • (ìµœì ê°’)
| íŒŒë¼ë¯¸í„° | ê°’ |
|---------|-----|
| Temperature | 0.4 |
| Repetition Penalty | 1.1 |
| Repetition Range | 64 |
| Top-k | 40 |
| Top-p | 0.9 |
| Min-p | 0.05 |

## ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# .env íŒŒì¼ì—ì„œ ì„¤ì •
AI_LOW_VRAM=true
AI_DTYPE=fp16  # ë˜ëŠ” ë” ë‚®ì€ ì •ë°€ë„
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# Hugging Face í† í° ì„¤ì •
export HF_TOKEN=your_token_here
```

### ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜
```bash
# PostgreSQL ì„œë¹„ìŠ¤ í™•ì¸
sudo systemctl status postgresql

# ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
createdb moonvoice
```

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- ğŸ“š [ê³µì‹ ë¬¸ì„œ](https://github.com/edwko/OuteTTS/blob/main/docs/interface_usage.md)
- ğŸ¤— [Hugging Face ëª¨ë¸](https://huggingface.co/OuteAI/OuteTTS-1.0-0.6B)
- ğŸ’¬ [Discord ì»¤ë®¤ë‹ˆí‹°](https://discord.gg/vyBM87kAmf)

---

# moonvoice
